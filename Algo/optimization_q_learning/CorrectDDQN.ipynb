{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wcZWFVfGxR0x"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import numba\n",
        "import numpy.random\n",
        "from collections import deque\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Reshape\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GridWorld:\n",
        "    def __init__(self, taille=[6, 6], position_start=[0, 1], good_end_position=[4, 2], bad_end_position=[3, 5]):\n",
        "        self.current_state = position_start  # État actuel (ligne, colonne)\n",
        "        self.states = [[x, y] for x in range(taille[0]) for y in range(taille[1])]\n",
        "        self.end_good_state = good_end_position  # État final (ligne, colonne)\n",
        "        self.end_bad_state = bad_end_position\n",
        "        self.grid_size = taille  # Taille de la grille (lignes, colonnes)\n",
        "        self.stateSpace = {}\n",
        "        self.matchStates()\n",
        "        self.currentIntState = self.getStateInt(self.current_state)\n",
        "        self.num_actions = 4  # Nombre total d'actions possibles (haut, bas, gauche, droite)\n",
        "        self.reward = 0  # Récompense actuelle\n",
        "        self.done = False  # Indique si la partie est terminée\n",
        "        self.generate_grid()\n",
        "        self.actions = [0, 1, 2, 3]\n",
        "        self.rewards = [0, 1, 3]\n",
        "        self.actionSpace = {0: -self.grid_size[0], 1: self.grid_size[0],\n",
        "                            2: -1, 3: 1}\n",
        "\n",
        "    def state_description(self):\n",
        "        return np.array([self.currentIntState / (len(self.states) - 1) * 2.0 - 1.0])\n",
        "\n",
        "    def state_dim(self):\n",
        "        return len(self.state_description())\n",
        "\n",
        "    def reset(self):\n",
        "        self.done = False\n",
        "        self.current_state = [np.random.randint(0, self.grid_size[0] - 1), np.random.randint(0, self.grid_size[1] - 1)]\n",
        "        self.currentIntState = self.getStateInt(self.current_state)\n",
        "        self.reward = 0\n",
        "\n",
        "    def matchStates(self):\n",
        "        i = 0\n",
        "        for s in self.states:\n",
        "            self.stateSpace[str(s)] = i\n",
        "            i = i + 1\n",
        "\n",
        "    def getStateInt(self, st):\n",
        "        return self.stateSpace[str(st)]\n",
        "\n",
        "    def getStateCouple(self, st):\n",
        "        n_state = {i for i in self.stateSpace if self.stateSpace[i] == st}\n",
        "        return list(n_state)\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 0:\n",
        "            if self.current_state[0] == 0:\n",
        "                self.current_state[0] = self.grid_size[0] - 1\n",
        "                self.currentIntState = self.getStateInt(self.current_state)\n",
        "                self.reward = 0  # Pas de récompense pour traverser le mur\n",
        "                self.generate_grid()\n",
        "                self.endgame()\n",
        "            else:\n",
        "                self.current_state[0] = self.current_state[0] - 1\n",
        "                self.currentIntState = self.getStateInt(self.current_state)\n",
        "                self.reward = 0  # Pas de récompense pour avancer\n",
        "                self.generate_grid()\n",
        "                self.endgame()\n",
        "\n",
        "        elif action == 1:\n",
        "            if self.current_state[0] == self.grid_size[0] - 1:\n",
        "                self.current_state[0] = 0\n",
        "                self.currentIntState = self.getStateInt(self.current_state)\n",
        "                self.reward = 0  # Pas de récompense pour avancer\n",
        "                self.generate_grid()\n",
        "                self.endgame()\n",
        "            else:\n",
        "                self.current_state[0] = self.current_state[0] + 1\n",
        "                self.currentIntState = self.getStateInt(self.current_state)\n",
        "                self.reward = 0  # Pas de récompense pour avancer\n",
        "                self.generate_grid()\n",
        "                self.endgame()\n",
        "\n",
        "        elif action == 2:\n",
        "            if self.current_state[1] == 0:\n",
        "                self.current_state[1] = self.grid_size[1] - 1\n",
        "                self.currentIntState = self.getStateInt(self.current_state)\n",
        "                self.reward = 0  # Pas de récompense pour avancer\n",
        "                self.generate_grid()\n",
        "                self.endgame()\n",
        "            else:\n",
        "                self.current_state[1] = self.current_state[1] - 1\n",
        "                self.currentIntState = self.getStateInt(self.current_state)\n",
        "                self.reward = 0  # Pas de récompense pour avancer\n",
        "                self.generate_grid()\n",
        "                self.endgame()\n",
        "\n",
        "        elif action == 3:\n",
        "            if self.current_state[1] == self.grid_size[1] - 1:\n",
        "                self.current_state[1] = 0\n",
        "                self.currentIntState = self.getStateInt(self.current_state)\n",
        "                # print(self.current_state)\n",
        "                self.reward = 0  # Pas de récompense pour avancer\n",
        "                self.generate_grid()\n",
        "                self.endgame()\n",
        "            else:\n",
        "                self.current_state[1] = self.current_state[1] + 1\n",
        "                self.currentIntState = self.getStateInt(self.current_state)\n",
        "                self.reward = 0  # Pas de récompense pour avancer\n",
        "                self.generate_grid()\n",
        "                self.endgame()\n",
        "                # Si l'on atteint l'état final, la partie est terminée\n",
        "        return self.currentIntState, self.reward, self.done\n",
        "\n",
        "    def endgame(self):\n",
        "        if self.current_state == self.end_good_state:\n",
        "            self.reward = 10  # Récompense de 10 pour atteindre l'état final\n",
        "            self.done = True\n",
        "            print(\"Une bonne récompense\")\n",
        "        elif self.current_state == self.end_bad_state:\n",
        "            self.reward = -10\n",
        "            self.done = True\n",
        "            print(\"Une mauvaise récompense\")\n",
        "\n",
        "    def generate_grid(self):\n",
        "        grid = []\n",
        "        for i in range(self.grid_size[0]):\n",
        "            grid.append([])\n",
        "            for j in range(self.grid_size[1]):\n",
        "                grid[i].append(\"_\")\n",
        "        grid[self.current_state[0]][self.current_state[1]] = \"X\"\n",
        "        for i in grid:\n",
        "            print(i)\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "j6mGy5msxWAO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_compile_model(env):\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(Dense(24, input_dim=1, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(len(env.actions), activation='linear'))\n",
        "\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.01))\n",
        "    return model"
      ],
      "metadata": {
        "id": "erM45s56xXU2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ddqn(env, episodes=1000, gamma=0.99, alpha=0.1, epsilon=0.1):\n",
        "    q_network = build_compile_model(env)\n",
        "    target_network = build_compile_model(env)\n",
        "    nb_steps = 0\n",
        "    first_episode = True\n",
        "    \n",
        "    step = 0\n",
        "    reward = 0\n",
        "    reward_per_episode = []\n",
        "    steps_per_episode = []\n",
        "    steps_by_episode = []\n",
        "    batch_size = 32\n",
        "    memory = deque(maxlen=2000)\n",
        "    \n",
        "    for episode in range(episodes):\n",
        "        env.reset()\n",
        "        print(\"Episode : \", episode)\n",
        "        cumumated_reward = 0\n",
        "        done = False\n",
        "        current_state = env.currentIntState\n",
        "        while not done:\n",
        "            actions = env.actions\n",
        "\n",
        "            q_values = q_network.predict(np.array([current_state]))[0]\n",
        "            if np.random.rand() < epsilon:\n",
        "                a = np.random.choice(actions)\n",
        "            else:\n",
        "                a= np.argmax(q_values)\n",
        "\n",
        "            old_reward = env.reward\n",
        "            new_state, reward, done = env.step(a)\n",
        "            #new_state = env.state_description()\n",
        "            memory.append((old_reward, a, reward, new_state, done))\n",
        "            \n",
        "            \"\"\"if (len(memory) > batch_size):\n",
        "                minibatch = random.sample(memory, batch_size)\n",
        "                for s, ac, reward, ns, terminated in minibatch:\n",
        "                    q_values = q_network.predict(np.array([s]))[0]\"\"\"\n",
        "            if done:\n",
        "                q_values[a] = reward\n",
        "            else:\n",
        "                t = target_network.predict(np.array([new_state]))[0]\n",
        "                q_values[a] = reward + gamma * np.amax(t)\n",
        "            q_network.fit(np.array([current_state]), np.array([q_values]), verbose=0)\n",
        "            \n",
        "            cumumated_reward += reward\n",
        "            step += 1\n",
        "\n",
        "            if done:\n",
        "                target_network.set_weights(q_network.get_weights())\n",
        "                \n",
        "        reward_per_episode.append(cumumated_reward)\n",
        "        steps_by_episode.append(step)\n",
        "        print(reward_per_episode)\n",
        "    return reward_per_episode, steps_by_episode, q_network, target_network"
      ],
      "metadata": {
        "id": "jg_vrAsnxbd4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_ddqn(env,model,target, episodes=1, gamma=0.99, alpha=0.1, epsilon=0.1):\n",
        "    q_network = model\n",
        "    target_network = target\n",
        "    nb_steps = 0\n",
        "    first_episode = True\n",
        "    \n",
        "    step = 0\n",
        "    reward = 0\n",
        "    reward_per_episode = []\n",
        "    steps_per_episode = []\n",
        "    steps_by_episode = []\n",
        "    batch_size = 32\n",
        "    memory = deque(maxlen=2000)\n",
        "    \n",
        "    for episode in range(episodes):\n",
        "        env.reset()\n",
        "        print(\"Episode : \", episode)\n",
        "        cumumated_reward = 0\n",
        "        done = False\n",
        "        current_state = env.currentIntState\n",
        "        while not done:\n",
        "            actions = env.actions\n",
        "\n",
        "            q_values = q_network.predict(np.array([current_state]))[0]\n",
        "            if np.random.rand() < epsilon:\n",
        "                a = np.random.choice(actions)\n",
        "            else:\n",
        "                a= np.argmax(q_values)\n",
        "\n",
        "            old_reward = env.reward\n",
        "            new_state, reward, done = env.step(a)\n",
        "            #new_state = env.state_description()\n",
        "            memory.append((old_reward, a, reward, new_state, done))\n",
        "            \n",
        "            \"\"\"if (len(memory) > batch_size):\n",
        "                minibatch = random.sample(memory, batch_size)\n",
        "                for s, ac, reward, ns, terminated in minibatch:\n",
        "                    q_values = q_network.predict(np.array([s]))[0]\"\"\"\n",
        "            if done:\n",
        "                q_values[a] = reward\n",
        "            else:\n",
        "                t = target_network.predict(np.array([new_state]))[0]\n",
        "                q_values[a] = reward + gamma * np.amax(t)\n",
        "            q_network.fit(np.array([current_state]), np.array([q_values]), verbose=0)\n",
        "            \n",
        "            cumumated_reward += reward\n",
        "            step += 1\n",
        "\n",
        "            if done:\n",
        "                target_network.set_weights(q_network.get_weights())\n",
        "                \n",
        "        reward_per_episode.append(cumumated_reward)\n",
        "        steps_by_episode.append(step)\n",
        "        print(reward_per_episode)\n",
        "    return reward_per_episode, steps_by_episode"
      ],
      "metadata": {
        "id": "cQCku6T6xkaC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    world = GridWorld()\n",
        "    scores, steps, model, target = ddqn(world, episodes = 1000)\n",
        "    plt.plot(scores)\n",
        "    plt.show()\n",
        "    plt.plot(steps)\n",
        "    plt.show()\n",
        "    test_scores, test_steps, = test_ddqn(world,model,target, episodes = 1)\n",
        "    print(test_scores, test_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "2Jjfs68vxdGI",
        "outputId": "b65ffa1c-3042-4b03-f0cb-c9bf01a86bd8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e2c79c757a13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mworld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridWorld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'GridWorld' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7yjQdXP2yXbs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}