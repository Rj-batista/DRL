{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e432126d-7fb3-46b2-b344-fcaec237e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import numba\n",
    "import numpy.random\n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c0b94-cab2-40ae-88ac-df26a67171a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, taille=[6, 6], position_start=[0, 1], good_end_position=[4, 2], bad_end_position=[3, 5]):\n",
    "        self.current_state = position_start  # État actuel (ligne, colonne)\n",
    "        self.states = [[x, y] for x in range(taille[0]) for y in range(taille[1])]\n",
    "        self.end_good_state = good_end_position  # État final (ligne, colonne)\n",
    "        self.end_bad_state = bad_end_position\n",
    "        self.grid_size = taille  # Taille de la grille (lignes, colonnes)\n",
    "        self.stateSpace = {}\n",
    "        self.matchStates()\n",
    "        self.currentIntState = self.getStateInt(self.current_state)\n",
    "        self.num_actions = 4  # Nombre total d'actions possibles (haut, bas, gauche, droite)\n",
    "        self.reward = 0  # Récompense actuelle\n",
    "        self.done = False  # Indique si la partie est terminée\n",
    "        self.generate_grid()\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "        self.rewards = [0, 1, 3]\n",
    "        self.actionSpace = {0: -self.grid_size[0], 1: self.grid_size[0],\n",
    "                            2: -1, 3: 1}\n",
    "\n",
    "    def state_description(self):\n",
    "        return np.array([self.currentIntState / (len(self.states) - 1) * 2.0 - 1.0])\n",
    "\n",
    "    def state_dim(self):\n",
    "        return len(self.state_description())\n",
    "\n",
    "    def reset(self):\n",
    "        self.done = False\n",
    "        self.current_state = [np.random.randint(0, self.grid_size[0] - 1), np.random.randint(0, self.grid_size[1] - 1)]\n",
    "        self.currentIntState = self.getStateInt(self.current_state)\n",
    "        self.reward = 0\n",
    "\n",
    "    def matchStates(self):\n",
    "        i = 0\n",
    "        for s in self.states:\n",
    "            self.stateSpace[str(s)] = i\n",
    "            i = i + 1\n",
    "\n",
    "    def getStateInt(self, st):\n",
    "        return self.stateSpace[str(st)]\n",
    "\n",
    "    def getStateCouple(self, st):\n",
    "        n_state = {i for i in self.stateSpace if self.stateSpace[i] == st}\n",
    "        return list(n_state)\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            if self.current_state[0] == 0:\n",
    "                self.current_state[0] = self.grid_size[0] - 1\n",
    "                self.currentIntState = self.getStateInt(self.current_state)\n",
    "                self.reward = 0  # Pas de récompense pour traverser le mur\n",
    "                self.generate_grid()\n",
    "                self.endgame()\n",
    "            else:\n",
    "                self.current_state[0] = self.current_state[0] - 1\n",
    "                self.currentIntState = self.getStateInt(self.current_state)\n",
    "                self.reward = 0  # Pas de récompense pour avancer\n",
    "                self.generate_grid()\n",
    "                self.endgame()\n",
    "\n",
    "        elif action == 1:\n",
    "            if self.current_state[0] == self.grid_size[0] - 1:\n",
    "                self.current_state[0] = 0\n",
    "                self.currentIntState = self.getStateInt(self.current_state)\n",
    "                self.reward = 0  # Pas de récompense pour avancer\n",
    "                self.generate_grid()\n",
    "                self.endgame()\n",
    "            else:\n",
    "                self.current_state[0] = self.current_state[0] + 1\n",
    "                self.currentIntState = self.getStateInt(self.current_state)\n",
    "                self.reward = 0  # Pas de récompense pour avancer\n",
    "                self.generate_grid()\n",
    "                self.endgame()\n",
    "\n",
    "        elif action == 2:\n",
    "            if self.current_state[1] == 0:\n",
    "                self.current_state[1] = self.grid_size[1] - 1\n",
    "                self.currentIntState = self.getStateInt(self.current_state)\n",
    "                self.reward = 0  # Pas de récompense pour avancer\n",
    "                self.generate_grid()\n",
    "                self.endgame()\n",
    "            else:\n",
    "                self.current_state[1] = self.current_state[1] - 1\n",
    "                self.currentIntState = self.getStateInt(self.current_state)\n",
    "                self.reward = 0  # Pas de récompense pour avancer\n",
    "                self.generate_grid()\n",
    "                self.endgame()\n",
    "\n",
    "        elif action == 3:\n",
    "            if self.current_state[1] == self.grid_size[1] - 1:\n",
    "                self.current_state[1] = 0\n",
    "                self.currentIntState = self.getStateInt(self.current_state)\n",
    "                # print(self.current_state)\n",
    "                self.reward = 0  # Pas de récompense pour avancer\n",
    "                self.generate_grid()\n",
    "                self.endgame()\n",
    "            else:\n",
    "                self.current_state[1] = self.current_state[1] + 1\n",
    "                self.currentIntState = self.getStateInt(self.current_state)\n",
    "                self.reward = 0  # Pas de récompense pour avancer\n",
    "                self.generate_grid()\n",
    "                self.endgame()\n",
    "                # Si l'on atteint l'état final, la partie est terminée\n",
    "        return self.currentIntState, self.reward, self.done\n",
    "\n",
    "    def endgame(self):\n",
    "        if self.current_state == self.end_good_state:\n",
    "            self.reward = 10  # Récompense de 10 pour atteindre l'état final\n",
    "            self.done = True\n",
    "            print(\"Une bonne récompense\")\n",
    "        elif self.current_state == self.end_bad_state:\n",
    "            self.reward = -10\n",
    "            self.done = True\n",
    "            print(\"Une mauvaise récompense\")\n",
    "\n",
    "    def generate_grid(self):\n",
    "        grid = []\n",
    "        for i in range(self.grid_size[0]):\n",
    "            grid.append([])\n",
    "            for j in range(self.grid_size[1]):\n",
    "                grid[i].append(\"_\")\n",
    "        grid[self.current_state[0]][self.current_state[1]] = \"X\"\n",
    "        for i in grid:\n",
    "            print(i)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673356c4-cb12-4094-80d9-ce6f73259157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_compile_model(env):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(Dense(24, input_dim=1, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(len(env.actions), activation='linear'))\n",
    "\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.01))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe1aa9-eb2f-45f5-acdf-9960d6e4b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(q_network, target_network, memory, batch_size, gamma):\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    for s, ac, rreward, ns, terminated in minibatch:\n",
    "        q_values = q_network.predict(np.array([s]))[0]\n",
    "        if done:\n",
    "            q_values[a] = rreward\n",
    "        else:\n",
    "            t = target_network.predict(np.array([new_state]))[0]\n",
    "        q_values[a] = rreward + gamma * np.amax(t)\n",
    "        q_network.fit(np.array([s]), np.array([q_values]), verbose=0)\n",
    "    \n",
    "def ddqn_er(env, episodes=1000, gamma=0.99, alpha=0.1, epsilon=0.1):\n",
    "    q_network = build_compile_model(env)\n",
    "    target_network = build_compile_model(env)\n",
    "    nb_steps = 0\n",
    "    first_episode = True\n",
    "    \n",
    "    step = 0\n",
    "    reward = 0\n",
    "    reward_per_episode = []\n",
    "    steps_by_episode = ()\n",
    "    step_by_episode = []\n",
    "    batch_size = 32\n",
    "    memory = deque(maxlen=2000)\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        env.reset()\n",
    "        print(\"Episode : \", episode)\n",
    "        cumumated_reward = 0\n",
    "        done = False\n",
    "        current_state = env.currentIntState\n",
    "        while not done:\n",
    "            actions = env.actions\n",
    "\n",
    "            q_values = q_network.predict(np.array([current_state]))[0]\n",
    "            if np.random.rand() < epsilon:\n",
    "                a = np.random.choice(actions)\n",
    "            else:\n",
    "                a= np.argmax(q_values)\n",
    "\n",
    "            old_reward = env.reward\n",
    "            new_state, reward, done = env.step(a)\n",
    "            #new_state = env.state_description()\n",
    "            memory.append((old_reward, a, reward, new_state, done))\n",
    "            \n",
    "            if (len(memory) > batch_size):\n",
    "                train_step(q_network, target_network, memory, batch_size, gamma)\n",
    "                \n",
    "            cumumated_reward += reward\n",
    "            step += 1\n",
    "\n",
    "            if done:\n",
    "                target_network.set_weights(q_network.get_weights())\n",
    "                \n",
    "        reward_per_episode.append(cumumated_reward)\n",
    "        step_by_episode.append(step)\n",
    "        print(reward_per_episode)\n",
    "    return reward_per_episode, step_by_episode, q_network, target_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f8892-22fe-4cac-8fde-e38c9a8d0f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    world = GridWorld()\n",
    "    scores, steps, model, target = ddqn_er(world, episodes = 1000)\n",
    "    plt.plot(scores)\n",
    "    plt.show()\n",
    "    plt.plot(steps)\n",
    "    plt.show()\n",
    "    test_scores, test_steps = test_ddqn_er(world,model,target, episodes = 1)\n",
    "    print(test_scores, test_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
