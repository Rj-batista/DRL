{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76628efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a86a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, taille, position_start, good_end_position, bad_end_position):\n",
    "        self.current_state = position_start  # État actuel (ligne, colonne)\n",
    "        self.states = [[x, y] for x in range(taille[0]) for y in range(taille[1])]\n",
    "        self.end_good_state = good_end_position  # État final (ligne, colonne)\n",
    "        self.end_bad_state = bad_end_position\n",
    "        self.grid_size = taille  # Taille de la grille (lignes, colonnes)\n",
    "        self.stateSpace = {}\n",
    "        self.matchStates()\n",
    "        self.currentIntState = self.getStateInt(self.current_state)\n",
    "        self.num_actions = 4  # Nombre total d'actions possibles (haut, bas, gauche, droite)\n",
    "        self.reward = 0  # Récompense actuelle\n",
    "        self.done = False  # Indique si la partie est terminée\n",
    "        self.generate_grid()\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "        self.rewards = [0, 1, 3]\n",
    "        self.actionSpace = {0: -self.grid_size[0], 1: self.grid_size[0],\n",
    "                            2: -1, 3: 1}\n",
    "\n",
    "    \n",
    "    def reset(self, position_start):\n",
    "        self.current_state = position_start\n",
    "        self.currentIntState = self.getStateInt(position_start)\n",
    "        \n",
    "    def matchStates(self):\n",
    "        i=0\n",
    "        for s in self.states:\n",
    "            self.stateSpace[str(s)] = i\n",
    "            i = i+1\n",
    "    \n",
    "    def getStateInt(self, st):\n",
    "        return self.stateSpace[str(st)]\n",
    "    \n",
    "    def getStateCouple(self, st):\n",
    "        n_state = {i for i in self.stateSpace if self.stateSpace[i]==st}\n",
    "        return list(n_state)\n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            if self.current_state[0] == 0 :\n",
    "                self.current_state[0] = self.grid_size[0] - 1\n",
    "                self.currentIntState = self.getStateInt(self.current_state)\n",
    "                self.reward = -1  # Pas de récompense pour traverser le mur\n",
    "                #self.generate_grid()\n",
    "                self.endgame()\n",
    "            else:\n",
    "                self.current_state[0] = self.current_state[0] - 1\n",
    "                self.currentIntState = self.getStateInt(self.current_state)\n",
    "                self.reward = -1  # Pas de récompense pour avancer\n",
    "                #self.generate_grid()\n",
    "                self.endgame()\n",
    "\n",
    "        elif action == 1:\n",
    "            if self.current_state[0] == self.grid_size[0] - 1:\n",
    "                self.current_state[0] = 0\n",
    "                self.currentIntState = self.getStateInt(self.current_state)\n",
    "                self.reward = -1  # Pas de récompense pour avancer\n",
    "                #self.generate_grid()\n",
    "                self.endgame()\n",
    "            else :\n",
    "                self.current_state[0] = self.current_state[0] + 1\n",
    "                self.currentIntState = self.getStateInt(self.current_state)\n",
    "                self.reward = -1  # Pas de récompense pour avancer\n",
    "                #self.generate_grid()\n",
    "                self.endgame()\n",
    "\n",
    "        elif action == 2:\n",
    "            if self.current_state[1] == 0:\n",
    "                self.current_state[1] = self.grid_size[1] - 1\n",
    "                self.currentIntState = self.getStateInt(self.current_state)\n",
    "                self.reward = -1  # Pas de récompense pour avancer\n",
    "                #self.generate_grid()\n",
    "                self.endgame()\n",
    "            else:\n",
    "                self.current_state[1] = self.current_state[1] - 1\n",
    "                self.currentIntState = self.getStateInt(self.current_state)\n",
    "                self.reward = -1  # Pas de récompense pour avancer\n",
    "                #self.generate_grid()\n",
    "                self.endgame()\n",
    "\n",
    "        elif action == 3:\n",
    "            if self.current_state[1] == self.grid_size[1] - 1:\n",
    "                self.current_state[1] = 0\n",
    "                self.currentIntState = self.getStateInt(self.current_state)\n",
    "                #print(self.current_state)\n",
    "                self.reward = -1  # Pas de récompense pour avancer\n",
    "                #self.generate_grid()\n",
    "                self.endgame()\n",
    "            else:\n",
    "                self.current_state[1] = self.current_state[1] + 1\n",
    "                self.currentIntState = self.getStateInt(self.current_state)\n",
    "                self.reward = -1  # Pas de récompense pour avancer\n",
    "                #self.generate_grid()\n",
    "                self.endgame()\n",
    "                # Si l'on atteint l'état final, la partie est terminée\n",
    "        return self.current_state, self.reward, self.done\n",
    "\n",
    "    def endgame(self):\n",
    "        if self.current_state == self.end_good_state:\n",
    "            self.reward = 10  # Récompense de 1 pour atteindre l'état final\n",
    "            self.done = True\n",
    "        elif self.current_state == self.end_bad_state:\n",
    "            self.reward = -10\n",
    "            self.done = True\n",
    "\n",
    "    # def update_grid(self):\n",
    "    #     new_grid = [[\"_\", \"_\", \"_\", \"_\"],\n",
    "    #                 [\"_\", \"_\", \"_\", \"_\"],\n",
    "    #                 [\"_\", \"_\", \"_\", \"_\"],\n",
    "    #                 [\"_\", \"_\", \"_\", \"_\"]]\n",
    "    #     new_grid[self.current_state[0]][self.current_state[1]] = \"X\"\n",
    "    #     for i in new_grid:\n",
    "    #         print(i)\n",
    "\n",
    "    def generate_grid(self):\n",
    "        grid=[]\n",
    "        for i in range(self.grid_size[0]):\n",
    "            grid.append([])\n",
    "            for j in range(self.grid_size[1]):\n",
    "                grid[i].append(\"_\")\n",
    "        grid[self.current_state[0]][self.current_state[1]] = \"X\"\n",
    "        for i in grid:\n",
    "            print(i)\n",
    "        print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb342716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initP(grid):\n",
    "    P = {}\n",
    "    for state in grid.states:\n",
    "        st = grid.getStateInt(state)\n",
    "        for action in grid.actions:\n",
    "            grid.current_state = state\n",
    "            state_, reward_, done_ = grid.step(action)\n",
    "            stt = grid.getStateInt(state_)\n",
    "            P[(stt, reward_, st, action)] = 1\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19825620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(grid, V, policy, discount_factor=0.9):\n",
    "    theta=1e-6\n",
    "    P = initP(grid)\n",
    "    #Initialize the Value function \n",
    "    #V = np.zeros((world.grid_size[0],world.grid_size[1]))\n",
    "    \n",
    "    while True:\n",
    "        DELTA = 0\n",
    "        for s in grid.states:\n",
    "            i = s[0]\n",
    "            j = s[1]\n",
    "            m_state = grid.getStateInt(s)\n",
    "            old_V = V[i][j]\n",
    "            weight = 1 / len(policy[m_state])\n",
    "            for action in policy[m_state]:\n",
    "                total = 0\n",
    "                for key in P:\n",
    "                    (newState, reward, oldState, act) = key\n",
    "                    n_state = list(grid.getStateCouple(newState)[0])\n",
    "                    n_state = [int(n_state[1]), int(n_state[4])]\n",
    "                    if oldState == m_state and act == action:\n",
    "                        total += weight*P[key]*(reward+discount_factor*V[n_state[0]][n_state[1]])\n",
    "            V[i][j] = total\n",
    "            DELTA = max(DELTA, np.abs(old_V-V[i][j]))\n",
    "            \n",
    "        if DELTA < theta:\n",
    "            return V            \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9040386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(grid, V, policy, discount_factor=0.9):\n",
    "    policy_stable = True\n",
    "    newPolicy = {}\n",
    "    for s in grid.states:\n",
    "        i = s[0]\n",
    "        j = s[0]\n",
    "        m_state = grid.getStateInt(s)\n",
    "        old_actions = policy[m_state]\n",
    "        value = []\n",
    "        newAction = []\n",
    "        P = initP(grid)\n",
    "        for action in old_actions:\n",
    "            total = 0\n",
    "            weight = 1 / len(policy[m_state])\n",
    "            for key in P:\n",
    "                (newState, reward, oldState, act) = key\n",
    "                n_state = list(grid.getStateCouple(newState)[0])\n",
    "                n_state = [int(n_state[1]), int(n_state[4])]\n",
    "                if oldState == m_state and act == action:\n",
    "                    total += weight*P[key]*(reward+discount_factor*V[n_state[0]][n_state[1]])\n",
    "                    value.append(np.round(total, 2))\n",
    "                    newAction.append(action)  \n",
    "                    \n",
    "        value = np.array(value) #Get the list of gotten value actions in the current state\n",
    "        print(value)\n",
    "        best = np.where(value == value.max())[0] #Get the position of the best gotten value action\n",
    "        bestActions = [newAction[item] for item in best]\n",
    "        newPolicy[m_state] = bestActions\n",
    "        \n",
    "        if old_actions != bestActions:\n",
    "            policy_stable = False\n",
    "    return policy_stable, newPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c120cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(grid, discount_factor=0.9):\n",
    "    \n",
    "    #Initialize the policy\n",
    "    V = np.random.random((world.grid_size[0],world.grid_size[1]))\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace.values():\n",
    "        # equiprobable random strategy\n",
    "        policy[state] = grid.actions\n",
    "    print(\"Policy : \")\n",
    "    print(policy)\n",
    "    V = evaluate_policy(grid, V, policy, discount_factor=0.9)\n",
    "    print(\"V = \")\n",
    "    print(V)\n",
    "    \n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "        old_policy = policy.copy()\n",
    "        \n",
    "        print(\"Start of policy evaluation\")\n",
    "        #Evaluate the policy\n",
    "        V = evaluate_policy(grid, V, policy, discount_factor=0.9)\n",
    "        print(\"V = \")\n",
    "        print(V)\n",
    "        \n",
    "        print(\"Start of policy Improvement\")\n",
    "        #Improve the policy\n",
    "        policy_stable, policy =  policy_improvement(grid, V, policy, discount_factor=0.9)\n",
    "        print(\"Policy : \")\n",
    "        print(policy)\n",
    "    \n",
    "        \"\"\"if all(old_policy[s] == policy[s] for s in grid.stateSpace.values()):\n",
    "            break\"\"\"\n",
    "            \n",
    "    return policy\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d54f6e4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', '_', '_', '_']\n",
      "['_', '_', '_', '_']\n",
      "['_', '_', '_', '_']\n",
      "['_', '_', '_', '_']\n",
      "\n",
      "\n",
      "Policy : \n",
      "{0: [0, 1, 2, 3], 1: [0, 1, 2, 3], 2: [0, 1, 2, 3], 3: [0, 1, 2, 3], 4: [0, 1, 2, 3], 5: [0, 1, 2, 3], 6: [0, 1, 2, 3], 7: [0, 1, 2, 3], 8: [0, 1, 2, 3], 9: [0, 1, 2, 3], 10: [0, 1, 2, 3], 11: [0, 1, 2, 3], 12: [0, 1, 2, 3], 13: [0, 1, 2, 3], 14: [0, 1, 2, 3], 15: [0, 1, 2, 3]}\n",
      "V = \n",
      "[[-0.32258059 -0.32258062 -0.3225806  -0.32258058]\n",
      " [-0.32258056 -0.32258062 -0.32258055 -0.32258056]\n",
      " [-0.32258057 -0.32258061 -0.32258055 -3.2258062 ]\n",
      " [-0.32258059 -0.32258056 -0.32258056  3.22580627]]\n",
      "Start of policy evaluation\n",
      "V = \n",
      "[[-0.32258063 -0.32258064 -0.32258063 -0.32258063]\n",
      " [-0.32258063 -0.32258064 -0.32258062 -0.32258063]\n",
      " [-0.32258063 -0.32258064 -0.32258062 -3.22580639]\n",
      " [-0.32258063 -0.32258063 -0.32258063  3.22580641]]\n",
      "Start of policy Improvement\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[ 3.23 -0.32 -0.32 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[-0.32 -0.32 -3.23 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[-0.32 -3.23 -0.32 -3.23]\n",
      "[-0.32 -0.32  3.23 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[-3.23  3.23 -0.32  3.23]\n",
      "Policy : \n",
      "{0: [0, 1, 2, 3], 1: [0, 1, 2, 3], 2: [0, 1, 2, 3], 3: [0], 4: [0, 1, 2, 3], 5: [0, 1, 2, 3], 6: [0, 1, 2, 3], 7: [0, 1, 2, 3], 8: [0, 1, 3], 9: [0, 1, 2, 3], 10: [0, 1, 2, 3], 11: [0, 2], 12: [2], 13: [0, 1, 2, 3], 14: [0, 1, 2, 3], 15: [1, 3]}\n",
      "Start of policy evaluation\n",
      "V = \n",
      "[[-0.32258065 -0.32258065 -0.32258065 18.18181757]\n",
      " [-0.32258065 -0.32258065 -0.32258065 -0.32258065]\n",
      " [-0.47619048 -0.32258065 -0.32258065 -0.64516129]\n",
      " [18.18181757 -0.32258065 -0.32258065  9.09090878]]\n",
      "Start of policy Improvement\n",
      "[ 3.84 -0.32  3.84 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[18.18]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[ 3.84 -0.32 -0.32 -0.32]\n",
      "[-0.43 -0.48 -0.48]\n",
      "[-0.32 -0.32 -0.36 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[-0.65 -0.65]\n",
      "[18.18]\n",
      "[-0.32 -0.32  3.84 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[9.09 9.09]\n",
      "Policy : \n",
      "{0: [0, 2], 1: [0, 1, 2, 3], 2: [0, 1, 2, 3], 3: [0], 4: [0, 1, 2, 3], 5: [0, 1, 2, 3], 6: [0, 1, 2, 3], 7: [0], 8: [0], 9: [0, 1, 3], 10: [0, 1, 2, 3], 11: [0, 2], 12: [2], 13: [2], 14: [0, 1, 2, 3], 15: [1, 3]}\n",
      "Start of policy evaluation\n",
      "V = \n",
      "[[ 7.68181818 -0.32258065 -0.32258065 18.18181818]\n",
      " [-0.32258065 -0.32258065 -0.32258065 15.36363636]\n",
      " [-1.29032258 -0.4761902  -0.32258065 -0.64516129]\n",
      " [18.18181818 15.36363636 -0.32258065  9.09090909]]\n",
      "Start of policy Improvement\n",
      "[7.68 7.68]\n",
      "[ 3.21 -0.32  1.48 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[18.18]\n",
      "[ 1.48 -0.32  3.21 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[15.36]\n",
      "[-1.29]\n",
      "[-0.43 -0.48 -0.48]\n",
      "[-0.32 -0.32 -0.36 -0.32]\n",
      "[ 6.41 -0.65]\n",
      "[18.18]\n",
      "[15.36]\n",
      "[-0.32 -0.32  3.21 -0.32]\n",
      "[9.09 9.09]\n",
      "Policy : \n",
      "{0: [0, 2], 1: [0], 2: [0, 1, 2, 3], 3: [0], 4: [2], 5: [0, 1, 2, 3], 6: [0, 1, 2, 3], 7: [0], 8: [0], 9: [0], 10: [0, 1, 3], 11: [0], 12: [2], 13: [2], 14: [2], 15: [1, 3]}\n",
      "Start of policy evaluation\n",
      "V = \n",
      "[[ 7.68181818 12.82727273 -0.32258065 18.18181818]\n",
      " [12.82727273 -0.32258065 -0.32258065 15.36363636]\n",
      " [10.54454545 -1.29032258 -0.4761902  12.82727273]\n",
      " [18.18181818 15.36363636 12.82727273  9.09090909]]\n",
      "Start of policy Improvement\n",
      "[7.68 7.68]\n",
      "[12.83]\n",
      "[ 2.64 -0.32  2.64 -0.32]\n",
      "[18.18]\n",
      "[12.83]\n",
      "[ 2.64 -0.32  2.64 -0.32]\n",
      "[-0.32 -0.32 -0.32 -0.32]\n",
      "[15.36]\n",
      "[10.54]\n",
      "[-1.29]\n",
      "[-0.43 -0.48 -0.48]\n",
      "[12.83]\n",
      "[18.18]\n",
      "[15.36]\n",
      "[12.83]\n",
      "[9.09 9.09]\n",
      "Policy : \n",
      "{0: [0, 2], 1: [0], 2: [0, 2], 3: [0], 4: [2], 5: [0, 2], 6: [0, 1, 2, 3], 7: [0], 8: [0], 9: [0], 10: [0], 11: [0], 12: [2], 13: [2], 14: [2], 15: [1, 3]}\n",
      "Start of policy evaluation\n",
      "V = \n",
      "[[ 7.68181818 12.82727273  5.27227273 18.18181818]\n",
      " [12.82727273  5.27227273 -0.32258065 15.36363636]\n",
      " [10.54454545  3.74504545 -1.29032258 12.82727273]\n",
      " [18.18181818 15.36363636 12.82727273  9.09090909]]\n",
      "Start of policy Improvement\n",
      "[7.68 7.68]\n",
      "[12.83]\n",
      "[5.27 5.27]\n",
      "[18.18]\n",
      "[12.83]\n",
      "[5.27 5.27]\n",
      "[ 0.94 -0.32  0.94 -0.32]\n",
      "[15.36]\n",
      "[10.54]\n",
      "[3.75]\n",
      "[-1.29]\n",
      "[12.83]\n",
      "[18.18]\n",
      "[15.36]\n",
      "[12.83]\n",
      "[9.09 9.09]\n",
      "Policy : \n",
      "{0: [0, 2], 1: [0], 2: [0, 2], 3: [0], 4: [2], 5: [0, 2], 6: [0, 2], 7: [0], 8: [0], 9: [0], 10: [0], 11: [0], 12: [2], 13: [2], 14: [2], 15: [1, 3]}\n",
      "Start of policy evaluation\n",
      "V = \n",
      "[[ 7.68181818 12.82727273  5.27227273 18.18181818]\n",
      " [12.82727273  5.27227273  1.87252273 15.36363636]\n",
      " [10.54454545  3.74504545  0.68527045 12.82727273]\n",
      " [18.18181818 15.36363636 12.82727273  9.09090909]]\n",
      "Start of policy Improvement\n",
      "[7.68 7.68]\n",
      "[12.83]\n",
      "[5.27 5.27]\n",
      "[18.18]\n",
      "[12.83]\n",
      "[5.27 5.27]\n",
      "[1.87 1.87]\n",
      "[15.36]\n",
      "[10.54]\n",
      "[3.75]\n",
      "[0.69]\n",
      "[12.83]\n",
      "[18.18]\n",
      "[15.36]\n",
      "[12.83]\n",
      "[9.09 9.09]\n",
      "Policy : \n",
      "{0: [0, 2], 1: [0], 2: [0, 2], 3: [0], 4: [2], 5: [0, 2], 6: [0, 2], 7: [0], 8: [0], 9: [0], 10: [0], 11: [0], 12: [2], 13: [2], 14: [2], 15: [1, 3]}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Il faut choisir un nombre entre 0 et 3\n",
    "    # Si l'action est 0, déplacement vers le haut\n",
    "    # Si l'action est 1, déplacement vers le bas\n",
    "    # Si l'action est 2, déplacement vers la gauche\n",
    "    # Si l'action est 3, déplacement vers la droite\n",
    "    world = GridWorld([4,4],[0,0],[3,3],[2,3]) #taille, position de départ, fin de jeu reward positive, fin de jeu reward négative\n",
    "    policy_iteration(world, discount_factor=0.9)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1670350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%sql` not found.\n"
     ]
    }
   ],
   "source": [
    "%sql"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
