{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2be44544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23a59c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineWorld:\n",
    "    def __init__(self, nb_cells=10, start_cell=1, good_end_cell=9, bad_end_cell=0):\n",
    "        self.done = None\n",
    "        self.current_state = start_cell  # État actuel\n",
    "        self.end_good_state = good_end_cell  # État final\n",
    "        self.end_bad_state = bad_end_cell\n",
    "        self.reward = 0.0\n",
    "        self.num_states = nb_cells  # Nombre total d'états\n",
    "        self.states = [i for i in range(nb_cells)]\n",
    "        self.actions = [0, 1]\n",
    "        self.num_actions = 2  # Nombre total d'actions possibles\n",
    "        self.line_world = [\"_\"] * (self.num_states - 1)\n",
    "        self.line_world.insert(self.current_state, \"X\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.reward = 0.0\n",
    "        self.done = False\n",
    "        self.current_state = np.random.randint(1, 8)\n",
    "\n",
    "    def state_description(self):\n",
    "        return np.array([self.current_state / (self.num_states - 1) * 2.0 - 1.0])\n",
    "\n",
    "    def state_dim(self):\n",
    "        return len(self.state_description())\n",
    "\n",
    "    def step(self, action):\n",
    "        # Si l'action est 1, on avance à droite\n",
    "        if (action == 1) and (self.current_state != self.end_good_state) and (self.current_state != self.end_bad_state):\n",
    "            self.current_state += 1\n",
    "            self.reward = 0  # Pas de récompense pour avancer\n",
    "            self.line_world.remove(\"X\")\n",
    "            self.line_world.insert(self.current_state, \"X\")\n",
    "            print(self.line_world)\n",
    "        # Si l'action est 0, on avance à gauche\n",
    "        elif (action == 0) and (self.current_state != self.end_good_state) and (\n",
    "                self.current_state != self.end_bad_state):\n",
    "            self.current_state -= 1\n",
    "            self.reward = 0  # Pas de récompense pour avancer\n",
    "            self.line_world.remove(\"X\")\n",
    "            self.line_world.insert(self.current_state, \"X\")\n",
    "            print(self.line_world)\n",
    "        # Si l'on atteint l'état final, la partie est terminée\n",
    "        if self.current_state == self.end_good_state:\n",
    "            self.reward = 1  # Récompense de 1 pour atteindre l'état final\n",
    "            # print(self.line_world)\n",
    "            self.done = True\n",
    "        elif self.current_state == self.end_bad_state:\n",
    "            self.reward = -1\n",
    "            # print(self.line_world)\n",
    "            self.done = True\n",
    "        return self.current_state, self.reward, self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4faed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initP(line):\n",
    "    P = {}\n",
    "    for state in line.states:\n",
    "        st = line.current_state\n",
    "        for action in line.actions:\n",
    "            line.current_state = state\n",
    "            state_, reward_, done_ = line.step(action)\n",
    "            stt = line.current_state\n",
    "            P[(stt, reward_, st, action)] = 1\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "402cda96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, V, policy, discount_factor=0.9):\n",
    "    theta=1e-6\n",
    "    P = initP(env)\n",
    "    #Initialize the Value function \n",
    "    #V = np.zeros((world.grid_size[0],world.grid_size[1]))\n",
    "    \n",
    "    while True:\n",
    "        DELTA = 0\n",
    "        for s in env.states:\n",
    "            old_V = V[s]\n",
    "            weight = 1 / len(policy[s])\n",
    "            for action in policy[s]:\n",
    "                total = 0\n",
    "                for key in P:\n",
    "                    (newState, reward, oldState, act) = key\n",
    "                    if oldState == s and act == action:\n",
    "                        total += weight*P[key]*(reward+discount_factor*V[newState])\n",
    "            V[s] = total\n",
    "            DELTA = max(DELTA, np.abs(old_V-V[s]))\n",
    "            \n",
    "        if DELTA < theta:\n",
    "            return V            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5c47a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, V, policy, discount_factor=0.9):\n",
    "    policy_stable = True\n",
    "    newPolicy = {}\n",
    "    for s in env.states:\n",
    "        old_actions = policy[s]\n",
    "        value = []\n",
    "        newAction = []\n",
    "        P = initP(env)\n",
    "        for action in old_actions:\n",
    "            total = 0\n",
    "            weight = 1 / len(policy[s])\n",
    "            for key in P:\n",
    "                (newState, reward, oldState, act) = key\n",
    "                if oldState == s and act == action:\n",
    "                    total += weight*P[key]*(reward+discount_factor*V[newState])\n",
    "                    value.append(np.round(total, 2))\n",
    "                    newAction.append(action)  \n",
    "                    \n",
    "        value = np.array(value) #Get the list of gotten value actions in the current state\n",
    "        print(value)\n",
    "        best = np.where(value == value.max())[0] #Get the position of the best gotten value action\n",
    "        bestActions = [newAction[item] for item in best]\n",
    "        newPolicy[s] = bestActions\n",
    "        \n",
    "        if old_actions != bestActions:\n",
    "            policy_stable = False\n",
    "    return policy_stable, newPolicy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "733f3a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, discount_factor=0.9):\n",
    "    \n",
    "    #Initialize the policy\n",
    "    V = np.random.random((len(env.states),))\n",
    "    V[0] = 0\n",
    "    V[env.num_states-1] = 0\n",
    "    \n",
    "    policy = {}\n",
    "    for state in env.states:\n",
    "        # equiprobable random strategy\n",
    "        policy[state] = env.actions\n",
    "    print(\"Policy : \")\n",
    "    print(policy)\n",
    "    \n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "        old_policy = policy.copy()\n",
    "        \n",
    "        print(\"Start of policy evaluation\")\n",
    "        #Evaluate the policy\n",
    "        V = evaluate_policy(env, V, policy, discount_factor=0.9)\n",
    "        print(\"V = \")\n",
    "        print(V)\n",
    "        \n",
    "        print(\"Start of policy Improvement\")\n",
    "        #Improve the policy\n",
    "        policy_stable, policy =  policy_improvement(env, V, policy, discount_factor=0.9)\n",
    "        print(\"Policy : \")\n",
    "        print(policy)\n",
    "\n",
    "            \n",
    "    return policy\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa558236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy : \n",
      "{0: [0, 1], 1: [0, 1], 2: [0, 1], 3: [0, 1], 4: [0, 1], 5: [0, 1], 6: [0, 1], 7: [0, 1], 8: [0, 1], 9: [0, 1]}\n",
      "Start of policy evaluation\n",
      "['X', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "['_', '_', 'X', '_', '_', '_', '_', '_', '_', '_']\n",
      "['_', 'X', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "['_', '_', '_', 'X', '_', '_', '_', '_', '_', '_']\n",
      "['_', '_', 'X', '_', '_', '_', '_', '_', '_', '_']\n",
      "['_', '_', '_', '_', 'X', '_', '_', '_', '_', '_']\n",
      "['_', '_', '_', 'X', '_', '_', '_', '_', '_', '_']\n",
      "['_', '_', '_', '_', '_', 'X', '_', '_', '_', '_']\n",
      "['_', '_', '_', '_', 'X', '_', '_', '_', '_', '_']\n",
      "['_', '_', '_', '_', '_', '_', 'X', '_', '_', '_']\n",
      "['_', '_', '_', '_', '_', 'X', '_', '_', '_', '_']\n",
      "['_', '_', '_', '_', '_', '_', '_', 'X', '_', '_']\n",
      "['_', '_', '_', '_', '_', '_', 'X', '_', '_', '_']\n",
      "['_', '_', '_', '_', '_', '_', '_', '_', 'X', '_']\n",
      "['_', '_', '_', '_', '_', '_', '_', 'X', '_', '_']\n",
      "['_', '_', '_', '_', '_', '_', '_', '_', '_', 'X']\n",
      "V = \n",
      "[ 0.00339647 -0.49847159  0.00754836  0.01677476  0.03727789  0.08284039\n",
      "  0.18409039  0.40909039  0.90909039  0.90909039]\n",
      "Start of policy Improvement\n",
      "['X', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "['_', '_', 'X', '_', '_', '_', '_', '_', '_', '_']\n",
      "['_', 'X', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "['_', '_', '_', 'X', '_', '_', '_', '_', '_', '_']\n",
      "['_', '_', 'X', '_', '_', '_', '_', '_', '_', '_']\n",
      "['_', '_', '_', '_', 'X', '_', '_', '_', '_', '_']\n",
      "['_', '_', '_', 'X', '_', '_', '_', '_', '_', '_']\n",
      "['_', '_', '_', '_', '_', 'X', '_', '_', '_', '_']\n",
      "['_', '_', '_', '_', 'X', '_', '_', '_', '_', '_']\n",
      "['_', '_', '_', '_', '_', '_', 'X', '_', '_', '_']\n",
      "['_', '_', '_', '_', '_', 'X', '_', '_', '_', '_']\n",
      "['_', '_', '_', '_', '_', '_', '_', 'X', '_', '_']\n",
      "['_', '_', '_', '_', '_', '_', 'X', '_', '_', '_']\n",
      "['_', '_', '_', '_', '_', '_', '_', '_', 'X', '_']\n",
      "['_', '_', '_', '_', '_', '_', '_', 'X', '_', '_']\n",
      "['_', '_', '_', '_', '_', '_', '_', '_', '_', 'X']\n",
      "[-0.5  0. ]\n",
      "['X', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "['_', '_', 'X', '_', '_', '_', '_', '_', '_', '_']\n",
      "['_', 'X', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "['_', '_', '_', 'X', '_', '_', '_', '_', '_', '_']\n",
      "['_', '_', 'X', '_', '_', '_', '_', '_', '_', '_']\n",
      "['_', '_', '_', '_', 'X', '_', '_', '_', '_', '_']\n",
      "['_', '_', '_', 'X', '_', '_', '_', '_', '_', '_']\n",
      "['_', '_', '_', '_', '_', 'X', '_', '_', '_', '_']\n",
      "['_', '_', '_', '_', 'X', '_', '_', '_', '_', '_']\n",
      "['_', '_', '_', '_', '_', '_', 'X', '_', '_', '_']\n",
      "['_', '_', '_', '_', '_', 'X', '_', '_', '_', '_']\n",
      "['_', '_', '_', '_', '_', '_', '_', 'X', '_', '_']\n",
      "['_', '_', '_', '_', '_', '_', 'X', '_', '_', '_']\n",
      "['_', '_', '_', '_', '_', '_', '_', '_', 'X', '_']\n",
      "['_', '_', '_', '_', '_', '_', '_', 'X', '_', '_']\n",
      "['_', '_', '_', '_', '_', '_', '_', '_', '_', 'X']\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-f5d4559bfc88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mworld\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLineWorld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mpolicy_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-47-735f3b3dad6e>\u001b[0m in \u001b[0;36mpolicy_iteration\u001b[1;34m(env, discount_factor)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start of policy Improvement\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m#Improve the policy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mpolicy_stable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mpolicy_improvement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Policy : \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-27eed2eedc8d>\u001b[0m in \u001b[0;36mpolicy_improvement\u001b[1;34m(env, V, policy, discount_factor)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Get the list of gotten value actions in the current state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#Get the position of the best gotten value action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mbestActions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnewAction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mnewPolicy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbestActions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     37\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[0;32m     38\u001b[0m           initial=_NoValue, where=True):\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    world = LineWorld()\n",
    "    policy_iteration(world, discount_factor=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
